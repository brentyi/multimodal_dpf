{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "from lib import dpf\n",
    "from lib import localization_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some helpers for generating data\n",
    "###\n",
    "\n",
    "observer = localization_models.BeaconObserver()\n",
    "observer.add_beacon((5, 3), 0.2)\n",
    "observer.add_beacon((22, 8), 0.5)\n",
    "observer.add_beacon((12, -10), 0.5)\n",
    "\n",
    "# Simulation\n",
    "def simulate_trajectory(timesteps=100):\n",
    "    dynamics = localization_models.RobotDynamicsModel()\n",
    "\n",
    "    # Generate states, control inputs\n",
    "    states = [np.array([0., 0., 0.])]\n",
    "    controls = []\n",
    "    for _ in range(timesteps):\n",
    "        control = torch.from_numpy(np.random.uniform(\n",
    "            low=[0, -0.1], high=[0.4, 0.1], size=(2,)).astype(np.float32))\n",
    "        new_state = dynamics.forward(\n",
    "            torch.from_numpy(states[-1][np.newaxis, np.newaxis, :].astype(np.float32)), control[np.newaxis,:], noisy=True)\n",
    "\n",
    "        states.append(new_state[0,0].numpy())\n",
    "        controls.append(control.numpy())\n",
    "\n",
    "    # Remove initial state\n",
    "    states = states\n",
    "\n",
    "    # Generate observations from ground-truth states\n",
    "    observations = observer.forward(states)\n",
    "\n",
    "    return states, observations, controls\n",
    "\n",
    "# Dead-reckoning\n",
    "def dead_reckon(controls, initial_state=np.array([0., 0., 0.])):\n",
    "    dynamics = localization_models.RobotDynamicsModel()\n",
    "\n",
    "    states = [initial_state]\n",
    "    for control in controls:\n",
    "        new_state = dynamics.forward(\n",
    "            torch.from_numpy(states[-1][np.newaxis, :].astype(np.float32)), control, noisy=False)\n",
    "        states.append(new_state[0].numpy())\n",
    "    \n",
    "    # Remove initial state and return\n",
    "    return states\n",
    "\n",
    "# Visualization helper\n",
    "def plot_trajectories(*states_list, **states_dict):\n",
    "    import itertools\n",
    "    plt.figure()\n",
    "    for label, states in itertools.chain(states_dict.items(), enumerate(states_list)):\n",
    "        if label == \"particles\":\n",
    "            # Format for particles should be states, log_weights\n",
    "            assert len(states) == 2\n",
    "            states, log_weights = states\n",
    "            weights = np.exp(log_weights)\n",
    "            weights /= np.max(weights)\n",
    "\n",
    "            x = np.asarray(states).T[0]\n",
    "            y = np.asarray(states).T[1]\n",
    "            plt.scatter(x, y, label=label, c=weights)\n",
    "        else:\n",
    "            x = np.asarray(states).T[0]\n",
    "            y = np.asarray(states).T[1]\n",
    "            plt.scatter(x, y, label=label)\n",
    "\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate some training data\n",
    "#\n",
    "training_trajectories = []\n",
    "for _ in range(100):\n",
    "    # States, observations, controls\n",
    "    s, o, u = simulate_trajectory(80)\n",
    "    training_trajectories.append((np.asarray(s), np.asarray(o), np.asarray(u)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories(*[t[0] for t in training_trajectories[:2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize particle filter network\n",
    "use_cuda = True #torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "try:\n",
    "    del pfnet\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Deleted old model!\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "dynamics = localization_models.RobotDynamicsModel()\n",
    "measurements = localization_models.DeepBeaconMeasurementModel()\n",
    "# measurements = localization_models.BeaconMeasurementModel(observer)\n",
    "pfnet = dpf.ParticleFilterNetwork(dynamics, measurements, 1.0).to(device)\n",
    "\n",
    "true_measurement_model = localization_models.BeaconMeasurementModel(observer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def to_torch(x):\n",
    "    return torch.from_numpy(x.astype(np.float32)).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(pfnet.parameters())\n",
    "def set_lr(lr):\n",
    "    print(\"Setting learning rate to\", lr)\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "# Training function\n",
    "def train(training_trajectories, epochs, subsequence_length=20, minibatch_size=10):\n",
    "    pfnet.train()\n",
    "\n",
    "    # Some constants\n",
    "    traj_count = len(training_trajectories)\n",
    "    minibatches = traj_count // minibatch_size\n",
    "    state_dim = training_trajectories[0][0].shape[1]\n",
    "    obs_dim = training_trajectories[0][1].shape[1]\n",
    "    control_dim = training_trajectories[0][2].shape[1]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        traj_indices = np.random.choice(traj_count, size=traj_count)\n",
    "        for traj_minibatch in np.array_split(traj_indices, minibatches):\n",
    "\n",
    "            # Pull out minibatch trajectories\n",
    "            minibatch_states = []\n",
    "            minibatch_observations = []\n",
    "            minibatch_controls = []\n",
    "            for i in traj_minibatch:\n",
    "                traj = training_trajectories[i]\n",
    "                minibatch_states.append(traj[0])\n",
    "                minibatch_observations.append(traj[1])\n",
    "                minibatch_controls.append(traj[2])\n",
    "\n",
    "            # We assume all of our trajectories have the same length\n",
    "            traj_length = len(minibatch_observations[0])\n",
    "\n",
    "            # Generate list of overlapping subsequences\n",
    "            ## TODO: clean this mess up\n",
    "            subsequences = []\n",
    "            start = 0\n",
    "            end = start + subsequence_length\n",
    "            while end <= traj_length:\n",
    "                subsequences.append((start, end))\n",
    "                start += subsequence_length #// 2\n",
    "                end = start + subsequence_length\n",
    "            \n",
    "            # Initialize some constants...\n",
    "            # N := number of trajectories in our minibatch\n",
    "            # M := number of particles\n",
    "            N = len(minibatch_states)\n",
    "            M = 100\n",
    "\n",
    "            # First, initialize our particle state + weight tensors\n",
    "            particles = np.zeros((N, M, state_dim))\n",
    "            log_weights = np.ones((N, M)) * (-np.log(M))\n",
    "            for i, s in enumerate(minibatch_states):\n",
    "                # Set all particles to initial state\n",
    "                particles[i] = np.random.uniform([-1, -1, 0], [1, 1, 0], size=(M, state_dim)) #s[start][np.newaxis,:]\n",
    "\n",
    "                # TODO: this noise feels like a hack\n",
    "                #particles[i] += np.random.normal(0, (1, 1, 0.2), size=particles[i].shape)\n",
    "            particles = to_torch(particles)\n",
    "            log_weights = to_torch(log_weights)\n",
    "\n",
    "            # Optimize over each subsequence!\n",
    "            print(subsequences)\n",
    "            for start, end in subsequences:\n",
    "                \n",
    "                losses = []\n",
    "\n",
    "                for timestep in range(start, end):\n",
    "                    # Build observation tensor\n",
    "                    observations = np.zeros((N, obs_dim))\n",
    "                    for i, o in enumerate(minibatch_observations):\n",
    "                        observations[i] = o[timestep]\n",
    "                    observations = to_torch(observations)\n",
    "                    \n",
    "                    # Build control tensor\n",
    "                    controls = np.zeros((N, control_dim))\n",
    "                    for i, c in enumerate(minibatch_controls):\n",
    "                        controls[i] = c[timestep]\n",
    "                    controls = to_torch(controls)\n",
    "                    \n",
    "                    # Build labeled state tensor\n",
    "                    label_states = np.zeros((N, state_dim))\n",
    "                    for i, s in enumerate(minibatch_states):\n",
    "                        # Note the +1 to the index -- this is because we\n",
    "                        # include t=0 in the state list\n",
    "                        label_states[i] = s[timestep + 1]\n",
    "                    label_states = to_torch(label_states)\n",
    "                    \n",
    "                    # Propagate through particle filter\n",
    "                    best_states, particles, log_weights = pfnet.forward(\n",
    "                        particles, log_weights, observations, controls, resample=False)\n",
    "                    \n",
    "                    predicted = pfnet.measurement_model(observations, particles)\n",
    "                    true = true_measurement_model(observations, particles)\n",
    "                    weight_std = torch.std(log_weights, dim=[1])\n",
    "                    assert weight_std.shape == (N,)\n",
    "                    weight_std = torch.mean(weight_std)\n",
    "                    # print(weight_std.cpu().detach().numpy())\n",
    "                    # Compute loss\n",
    "                    loss = F.mse_loss(predicted, true)\n",
    "                    losses.append(loss)\n",
    "                    \n",
    "                    # Don't backprop through time\n",
    "                    particles = particles.detach()\n",
    "                    log_weights = log_weights.detach()\n",
    "\n",
    "                loss = torch.mean(torch.stack(losses))\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "\n",
    "                print(\"Training loss:\", loss.cpu().detach().numpy())\n",
    "            print(\">>>>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lr(3e-5)\n",
    "train(training_trajectories[0:1], epochs=50, subsequence_length=80, minibatch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_states = to_torch(\n",
    "    np.random.normal(0., 1., size=(1, 20, 3))\n",
    ")\n",
    "observations = to_torch(observer.forward(particle_states[0:1, 0].cpu().numpy()))\n",
    "measurements(observations, particle_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_models.BeaconMeasurementModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "true_states, observations, controls = training_trajectories[0]\n",
    "num_particles = 500\n",
    "particle_states = torch.FloatTensor(\n",
    "    [[true_states[0] for _ in range(num_particles)]]).to(device)\n",
    "particle_weights = torch.ones((1,num_particles)).to(device)\n",
    "\n",
    "end = 80\n",
    "states = []\n",
    "for control, observation in zip(controls[:end], observations[:end]):\n",
    "    # Type conversions\n",
    "    observation = to_torch(observation[np.newaxis,:])\n",
    "    control = to_torch(control[np.newaxis,:])\n",
    "\n",
    "    # Particle filter network: forward\n",
    "    best_state, particle_states, particle_weights = pfnet.forward(\n",
    "        particle_states, particle_weights, observation, control, resample=True)\n",
    "    # print (np.std(particle_weights.cpu().detach().numpy()), np.mean(particle_weights.cpu().detach().numpy()))\n",
    "\n",
    "    states.append(best_state.cpu().detach().numpy())\n",
    "pf_states = np.array(states)\n",
    "particle_states = particle_states[0].cpu().detach().numpy()\n",
    "particle_weights = particle_weights[0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trained network\")\n",
    "# print(particle_weights)\n",
    "plot_trajectories(\n",
    "    true=true_states,\n",
    "    pf=pf_states,\n",
    "    particles=(particle_states, particle_weights),\n",
    "    beacons=observer.locations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trained network\")\n",
    "plot_trajectories(\n",
    "    true=true_states,\n",
    "    pf=pf_states,\n",
    "    particles=(particle_states[0].cpu().numpy(), particle_weights[0].cpu().numpy()),\n",
    "    beacons=observer.locations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
